model: model.yaml
training:
  max_steps: 1000000
  disc_loss_momentum: 0.9
  gan_start_th: 0.01
  gan_stop_th: 0.1
  grad_norm_g: 10
  grad_norm_d: 1
  accumulate_grad_batches: 1
  gan_start_step: 0
data:
  dataloader:
    train_batch_size: 8
optim_g:
- optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-3
      weight_decay: 5e-2
  lr_scheduler:
    scheduler:
      class_path: timm.scheduler.CosineLRScheduler
      init_args:
        t_initial: ${training.max_steps}
        t_in_epochs: false
        warmup_t: 30000
        warmup_prefix: true
    interval: step
    frequency: 200
optim_d:
- optimizer:
    class_path: torch.optim.AdamW
    init_args:
      lr: 1e-4
      weight_decay: 5e-2
  lr_scheduler:
    scheduler:
      class_path: timm.scheduler.CosineLRScheduler
      init_args:
        t_initial: ${training.max_steps}
        t_in_epochs: false
        warmup_t: 30000
        warmup_prefix: true
    interval: step
    frequency: 200
