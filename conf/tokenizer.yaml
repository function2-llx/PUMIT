seed_everything: 42
model:
  class_path: pumt.tokenizer.VQGAN
  init_args:
    z_channels: 256
    embedding_dim: 256
    ed_kwargs:
      in_channels: 3
      z_channels: ${..z_channels}
      layer_channels: [128, 128, 256, 512]
      num_res_blocks: 2
    vq_kwargs:
      num_embeddings: 8192
      embedding_dim: ${..embedding_dim}
      mode: soft
    loss_kwargs:
      in_channels: ${..ed_kwargs.in_channels}
      gan_start_step: 1000
      quant_weight: 1e-2
    num_pre_downsamples: 1
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-4
        weight_decay: 5e-2
    disc_optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 1e-4
        weight_decay: 5e-2
    lr_scheduler_config:
      scheduler:
        class_path: timm.scheduler.CosineLRScheduler
        init_args:
          t_initial: ${trainer.max_steps}
          lr_min: 1e-6
          warmup_t: 1000
          warmup_prefix: true
      interval: step
      frequency: 100
    disc_lr_scheduler_config:
      scheduler:
        class_path: timm.scheduler.CosineLRScheduler
        init_args:
          t_initial: ${trainer.max_steps}
          lr_min: 1e-6
          warmup_t: 1000
          warmup_prefix: true
      interval: step
      frequency: 100
data:
  class_path: pumt.tokenizer.TokenizerDataModule
  init_args:
    dataset_weights:
      CheXpert: 0.1
      NIHChestX-ray: 0.1
    dl_conf:
      train_batch_size: 2
      num_train_steps: ${trainer.max_steps}
    trans_conf:
      train_num_tokens: 1024
      train_scale_x:
        min: 0.75
        max: 2.0
      train_tx:
        min: 12
        max: 16
trainer:
  precision: 16-mixed
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      name: tokenizer
      save_dir: output/tokenizer
      project: PUMT
  max_steps: 100000
  benchmark: true
  val_check_interval: 100
  check_val_every_n_epoch: null
