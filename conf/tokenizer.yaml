vqvae:
  z_channels: 256
  embedding_dim: 256
  ed_kwargs:
    in_channels: 3
    z_channels: ${..z_channels}
    layer_channels: [128, 128, 256, 512]
    num_res_blocks: 2
  vq_kwargs:
    num_embeddings: 8192
    embedding_dim: ${..embedding_dim}
    mode: soft
  num_pre_downsamples: 1
optimizer_g:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
    weight_decay: 5e-2
lr_scheduler_g:
  scheduler:
    class_path: timm.scheduler.CosineLRScheduler
    init_args:
      t_initial: ${training.max_steps}
      lr_min: 1e-6
      warmup_t: 0
      warmup_prefix: true
  interval: step
  frequency: 100
loss:
  quant_weight: 1e-2
  gan_warmup_steps: 10000
  max_perceptual_slices: 16
optimizer_d:
  class_path: torch.optim.AdamW
  init_args:
    lr: 1e-4
    weight_decay: 5e-2
lr_scheduler_d:
  scheduler:
    class_path: timm.scheduler.CosineLRScheduler
    init_args:
      t_initial: ${training.max_steps}
      lr_min: 1e-6
      warmup_t: 0
      warmup_prefix: true
  interval: step
  frequency: 100
data:
  dataset_weights:
    CheXpert: 0.1
    NIHChestX-ray: 0.1
  dl_conf:
    train_batch_size: 1
    num_train_steps: ${training.max_steps}
  trans_conf:
    train_tz: 2
    train_tx:
      min: 8
      max: 8
    train_scale_x:
      min: 0.75
      max: 2.0
training:
  max_steps: 1000
  max_norm_d: 2
#ckpt_path: pre-trained/vqgan-gumbel-f8.ckpt
